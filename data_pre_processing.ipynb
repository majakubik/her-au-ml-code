{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65bb8bf",
   "metadata": {},
   "source": [
    "### 0. Data load and setup\n",
    "Load the raw Mamun dataset, import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd96740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_RAW = PROJECT_ROOT / \"data_raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data_processed\"\n",
    "\n",
    "INPUT_FILE = DATA_RAW / \"mamun.csv\"\n",
    "OUTPUT_FILE = DATA_PROCESSED / \"mamun_HER_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07cc35e",
   "metadata": {},
   "source": [
    "### 1. Surface composition parsing\n",
    "Parse surface composition to extract elements A, B and their counts m, n from the string, handles pure-element surfaces setting A = B and m = 0. Preserves original order of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f23477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_surface_composition(comp: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse surfaceComposition:\n",
    "      - 'Pt3Ni' as A=Pt, n=3, B=Ni, m=1\n",
    "      - 'PtNi3' as A=Pt, n=1, B=Ni, m=3\n",
    "      - 'Pt' as A=Pt, n=1, B=Pt, m=0\n",
    "    \"\"\"\n",
    "    comp = str(comp)\n",
    "\n",
    "    m = re.match(r'^([A-Z][a-z]*)(\\d*)(?:([A-Z][a-z]*)(\\d*))?$', comp)\n",
    "\n",
    "    A, n_str, B, m_str = m.groups()\n",
    "\n",
    "    n = int(n_str) if n_str else 1\n",
    "\n",
    "    if B is None:\n",
    "        # pure element case, treat B as A, m=0\n",
    "        B = np.nan\n",
    "        m_val = 0\n",
    "    else:\n",
    "        m_val = int(m_str) if m_str else 1\n",
    "\n",
    "    return pd.Series({\"surf_A\": A, \"surf_B\": B, \"surf_n\": n, \"surf_m\": m_val,})\n",
    "\n",
    "df[[\"surf_A\", \"surf_B\", \"surf_n\", \"surf_m\"]] = df[\"surfaceComposition\"].apply(parse_surface_composition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758e06c",
   "metadata": {},
   "source": [
    "### 2. Filter entries with one H per unit cell\n",
    "Filter the dataset to only contain dillute hydrogen adsorption entries where products = '{'Hstar': 1}'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_H1 = df[\"products\"] == \"{'Hstar': 1}\"\n",
    "\n",
    "df = df[mask_H1].copy()\n",
    "print(df.shape)\n",
    "print(df[\"products\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e59f66",
   "metadata": {},
   "source": [
    "### 3. Site label processing\n",
    "Simplify sites strings like '{'H': 'bridge|A_A|B'}' into compact labels, drops rare 4fold sites. Generalize tilt site variants into main site type, and differentiates hollow sites into fcc- and bcc-hollow. With get_norm_label_from_sites create a normalized label that preserves the identity of first-layer atoms for feature calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_site(site_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify a Mamun site string like 'hollow|A_A_B|FCC' into a compact label.\n",
    "    \"\"\"\n",
    "    parts = site_str.split(\"|\")\n",
    "    head = parts[0].lower()\n",
    "\n",
    "    if head == \"hollow\" and len(parts) == 3:\n",
    "        return parts[2].lower()\n",
    "    elif head == \"hollow-tilt\":\n",
    "        return \"hollow-tilt\"\n",
    "    elif head in (\"top\", \"top-tilt\"):\n",
    "        return \"ontop\"\n",
    "    elif head in (\"bridge\", \"bridge-tilt\"):\n",
    "        return head\n",
    "    else:\n",
    "        return head\n",
    "\n",
    "\n",
    "def extract_and_simplify_sites_field(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Since 'sites' column is a stringified dictionary, extract the 'H' entry and simplify.\n",
    "    \"\"\"\n",
    "    d = ast.literal_eval(s) # parses the string into dictonary\n",
    "    raw = d.get(\"H\", \"\")\n",
    "    return simplify_site(raw)\n",
    "\n",
    "def get_norm_label_from_sites(s: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Build a normalized site label for H* that:\n",
    "    - encodes site type fcc/hcp/bridge/ontop\n",
    "    - encodes first-layer A/B environment\n",
    "    - ignores tilt and 2nd-layer atom differences\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        d = ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "    h = d.get(\"H\")\n",
    "\n",
    "    h = h.strip()\n",
    "    parts = h.split(\"|\")\n",
    "    head = parts[0].lower()\n",
    "\n",
    "    # hollow site type\n",
    "    if head.startswith(\"hollow\") and len(parts) >= 3:\n",
    "        first_layer = parts[1].replace(\"_\", \"\").upper()\n",
    "        kind = parts[2].lower()\n",
    "        # use counts of A/B (order-insensitive)\n",
    "        nA = first_layer.count(\"A\")\n",
    "        nB = first_layer.count(\"B\")\n",
    "        return f\"{kind}{'A'*nA}{'B'*nB}\"\n",
    "\n",
    "    # bridge site type\n",
    "    if head.startswith(\"bridge\") and len(parts) >= 2:\n",
    "        first_layer = parts[1].replace(\"_\", \"\").upper()\n",
    "        nA = first_layer.count(\"A\")\n",
    "        nB = first_layer.count(\"B\")\n",
    "        return f\"bridge{'A'*nA}{'B'*nB}\"\n",
    "\n",
    "    # ontop site type\n",
    "    if head in (\"top\", \"top-tilt\", \"ontop\") and len(parts) >= 2:\n",
    "        ch = parts[1].strip().upper()\n",
    "        if ch in (\"A\", \"B\"):\n",
    "            return f\"ontop{ch}\"\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aae623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cleaned site label for ML\n",
    "df[\"site_simple\"] = df[\"sites\"].apply(extract_and_simplify_sites_field).astype(\"category\")\n",
    "\n",
    "tilt_map = {\"bridge-tilt\": \"bridge\", \"hollow-tilt\": \"fcc\", \"top-tilt\": \"ontop\"}\n",
    "\n",
    "df[\"site_simple_collapsed\"] = df[\"site_simple\"].replace(tilt_map)\n",
    "\n",
    "# drop 4fold entries\n",
    "df = df[df[\"site_simple_collapsed\"] != \"4fold\"].copy()\n",
    "\n",
    "print(df.shape)\n",
    "print(df[\"site_simple_collapsed\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize label for grouping\n",
    "df[\"norm_label\"] = df[\"sites\"].apply(get_norm_label_from_sites)\n",
    "\n",
    "print(df[[\"site_simple_collapsed\", \"norm_label\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b9c00",
   "metadata": {},
   "source": [
    "### 4. Duplicate collapsing\n",
    "Collapse duplicates arising due to disregarding tilt- variants and second-layer atoms. Duplicates are defined as entries with the same canonicalized surface alloy (order-invariant), facet, and norm_label (same first-layer environment). reactionEnergy_eV is averaged across such duplicates and one representative row is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize alloy to make it order-invariant\n",
    "def canonicalize_alloy(df, Acol=\"surf_A\", Bcol=\"surf_B\", ncol=\"surf_n\", mcol=\"surf_m\", prefix=\"mam\"):\n",
    "    out = df.copy()\n",
    "\n",
    "    A = out[Acol].fillna(\"\").astype(str)\n",
    "    B = out[Bcol].fillna(\"\").astype(str)\n",
    "    n = pd.to_numeric(out[ncol], errors=\"coerce\").fillna(0).astype(int)\n",
    "    m = pd.to_numeric(out[mcol], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    do_swap = (A != \"\") & (B != \"\") & (A > B)\n",
    "\n",
    "    out[f\"{prefix}_canon_A\"] = np.where(do_swap, B, A)\n",
    "    out[f\"{prefix}_canon_B\"] = np.where(do_swap, A, B)\n",
    "    out[f\"{prefix}_canon_n\"] = np.where(do_swap, m, n)\n",
    "    out[f\"{prefix}_canon_m\"] = np.where(do_swap, n, m)\n",
    "    \n",
    "    return out\n",
    "\n",
    "df = canonicalize_alloy(df)\n",
    "\n",
    "# normalize facet\n",
    "if \"facet\" in df.columns:\n",
    "    df[\"facet\"] = df[\"facet\"].astype(str).str.replace(r\"[()\\s]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69434d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized groups for duplicate detection\n",
    "group_cols = [\"mam_canon_A\", \"mam_canon_B\", \"mam_canon_n\", \"mam_canon_m\", \"facet\", \"norm_label\"]\n",
    "\n",
    "def collapse_group(g: pd.DataFrame) -> pd.Series:\n",
    "    row = g.iloc[0].copy()\n",
    "    row[\"reactionEnergy_eV\"] = pd.to_numeric(g[\"reactionEnergy_eV\"], errors=\"coerce\").mean()\n",
    "    if \"id\" in g.columns:\n",
    "        row[\"id\"] = \",\".join(map(str, g[\"id\"].unique()))\n",
    "    return row\n",
    "\n",
    "sizes = df.groupby(group_cols, dropna=False).size()\n",
    "print(\"Duplicate groups to collapse:\", int((sizes > 1).sum()))\n",
    "\n",
    "df = (df.sort_values(group_cols)\n",
    "     .groupby(group_cols, dropna=False, as_index=False)\n",
    "     .apply(collapse_group, include_groups=False)\n",
    "     .reset_index(drop=True))\n",
    "\n",
    "# safety check\n",
    "assert df.groupby(group_cols, dropna=False).size().max() == 1\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc76cb",
   "metadata": {},
   "source": [
    "### 5. Save output\n",
    "Drop intermediate columns, save final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"sites_orig\", \"mam_canon_A\", \"mam_canon_B\", \"mam_canon_n\", \"mam_canon_m\"]\n",
    "\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
